{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The task in this notebook is to recreate the dataset used in the paper, by Kačar et al., 2023: Aircraft Accident Prediction Using Machine Learning Classification Algorithms.\n",
    "\n",
    "### Data preprocessing procedures:\t\n",
    "\n",
    "0. merge all tables into one spreadsheet;\n",
    "1. create the Severtiy Classes:\n",
    "1. remove ‘homebuilt’ == Yes and ‘acft_category’ != ‘Airplane’ \n",
    "2. remove ‘Severity Class’ == NaN\n",
    "3. remove column, if variance(column) == 0  (i.e. there is only a value in the column)\n",
    "4. check column, if variance(column) ~ 0 (near-zero): consider transforming those columns to Categorical;\n",
    "5. remove duplicated rows of data;\n",
    "6. change wrong ‘flags’ to NaNs: 999 for gust_kts, -1 for apr_dist, outside of [-180,180] for longitude\n",
    "7. IQR to remove data >99% (outliers);\n",
    "\n",
    "8. IMPORTANT: Split Now, to avoid data Leakage. Keep 10/20% in a Holdout Set for final evaluation.\n",
    "\n",
    "9.  Cross-Correlated features:\n",
    "    1. Numerical: Pearson correlation and Spearman correlation. Remove: wx_dev_pt, as it is correlated with wx_temp\n",
    "    2. Categorical: Phi-k correlation and Mutual Information. Nothing removed.\n",
    "10. Relationship between features and labels (?):\n",
    "    1. Numerical: ANOVA. Remove apt_dist, gust_kts, ev_time, ev_year, wind_dir_deg\n",
    "    2. Categorical: Mutual Information. Remove ev_dow, ev_month.\n",
    "11. Replace NaNs with median (numerical cols) and mode (categorical cols).\n",
    "12. MinMaxScaler Normalise;\n",
    "13. One-hot Encode Categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opening every sheet of the NTSB dataset, using pandas:\n",
    "\n",
    "This is not a common way to open datasets using pandas, but we need to do this for Excel files with multiple sheets.\n",
    "\n",
    "It is more common to do:\n",
    "\n",
    "``` dataframe = pd.read_excel(path_to_data)```, but this only reads the first sheet of the Excel file, if nothing else is specified.\n",
    "\n",
    "In the case of **.csv** files, which are the most common, we can use:\n",
    "\n",
    "```dataframe = pd.read_csv(path_to_data)```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relative path from the folder containing this notebook:\n",
    "path_to_data = '../data/NTSB avall file dataset.xlsx'\n",
    "\n",
    "# initialise the Excel file:\n",
    "ntsb = pd.ExcelFile(path_to_data)\n",
    "\n",
    "# read every sheet in the Excel file:\n",
    "ntsb1 = pd.read_excel(ntsb, 'Aircraft related')\n",
    "ntsb2 = pd.read_excel(ntsb, 'Event related')\n",
    "ntsb3 = pd.read_excel(ntsb, 'Event sequence')\n",
    "ntsb4 = pd.read_excel(ntsb, 'Findings')\n",
    "ntsb5 = pd.read_excel(ntsb, 'Engines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27844\n",
      "27410\n",
      "59327\n",
      "65093\n",
      "25266\n"
     ]
    }
   ],
   "source": [
    "print(len(ntsb1))\n",
    "print(len(ntsb2))\n",
    "print(len(ntsb3))\n",
    "print(len(ntsb4))\n",
    "print(len(ntsb5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_columns = list(set(ntsb1.columns) & set(ntsb2.columns))\n",
    "\n",
    "merged = pd.merge(ntsb1, ntsb2, how='inner', on=common_columns)\n",
    "# merged = pd.merge(ntsb1, ntsb2, how='left', on=common_columns)\n",
    "# merged = pd.merge(ntsb1, ntsb2, how='outer', on=common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on object and float64 columns. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(merged, ntsb2, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m, on\u001b[38;5;241m=\u001b[39mcommon_columns)\n\u001b[1;32m      6\u001b[0m common_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(merged\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(ntsb3\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[0;32m----> 7\u001b[0m merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(merged, ntsb3, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m, on\u001b[38;5;241m=\u001b[39mcommon_columns)\n\u001b[1;32m      9\u001b[0m common_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(merged\u001b[38;5;241m.\u001b[39mcolumns) \u001b[38;5;241m&\u001b[39m \u001b[38;5;28mset\u001b[39m(ntsb4\u001b[38;5;241m.\u001b[39mcolumns))\n\u001b[1;32m     10\u001b[0m merged \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mmerge(merged, ntsb4, how\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minner\u001b[39m\u001b[38;5;124m'\u001b[39m, on\u001b[38;5;241m=\u001b[39mcommon_columns)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/phd_main/lib/python3.11/site-packages/pandas/core/reshape/merge.py:110\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     94\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    108\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    109\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 110\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[1;32m    111\u001b[0m         left,\n\u001b[1;32m    112\u001b[0m         right,\n\u001b[1;32m    113\u001b[0m         how\u001b[38;5;241m=\u001b[39mhow,\n\u001b[1;32m    114\u001b[0m         on\u001b[38;5;241m=\u001b[39mon,\n\u001b[1;32m    115\u001b[0m         left_on\u001b[38;5;241m=\u001b[39mleft_on,\n\u001b[1;32m    116\u001b[0m         right_on\u001b[38;5;241m=\u001b[39mright_on,\n\u001b[1;32m    117\u001b[0m         left_index\u001b[38;5;241m=\u001b[39mleft_index,\n\u001b[1;32m    118\u001b[0m         right_index\u001b[38;5;241m=\u001b[39mright_index,\n\u001b[1;32m    119\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    120\u001b[0m         suffixes\u001b[38;5;241m=\u001b[39msuffixes,\n\u001b[1;32m    121\u001b[0m         indicator\u001b[38;5;241m=\u001b[39mindicator,\n\u001b[1;32m    122\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result(copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/phd_main/lib/python3.11/site-packages/pandas/core/reshape/merge.py:707\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, indicator, validate)\u001b[0m\n\u001b[1;32m    699\u001b[0m (\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    701\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[1;32m    703\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_merge_keys()\n\u001b[1;32m    705\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    706\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[0;32m--> 707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n\u001b[1;32m    709\u001b[0m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[1;32m    712\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/phd_main/lib/python3.11/site-packages/pandas/core/reshape/merge.py:1340\u001b[0m, in \u001b[0;36m_MergeOperation._maybe_coerce_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[1;32m   1336\u001b[0m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1337\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   1338\u001b[0m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[1;32m   1339\u001b[0m     ):\n\u001b[0;32m-> 1340\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1342\u001b[0m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk\u001b[38;5;241m.\u001b[39mdtype):\n",
      "\u001b[0;31mValueError\u001b[0m: You are trying to merge on object and float64 columns. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "merged = ntsb1.copy()\n",
    "\n",
    "common_columns = list(set(merged.columns) & set(ntsb2.columns))\n",
    "merged = pd.merge(merged, ntsb2, how='inner', on=common_columns)\n",
    "\n",
    "common_columns = list(set(merged.columns) & set(ntsb3.columns))\n",
    "merged = pd.merge(merged, ntsb3, how='inner', on=common_columns)\n",
    "\n",
    "common_columns = list(set(merged.columns) & set(ntsb4.columns))\n",
    "merged = pd.merge(merged, ntsb4, how='inner', on=common_columns)\n",
    "\n",
    "common_columns = list(set(merged.columns) & set(ntsb5.columns))\n",
    "merged = pd.merge(merged, ntsb5, how='inner', on=common_columns)\n",
    "\n",
    "# for df in [ntsb2, ntsb3, ntsb4, ntsb5]:\n",
    "#     common_columns = list(set(merged.columns) & set(df.columns))\n",
    "#     print(common_columns)\n",
    "#     merged = pd.merge(merged, df, how='inner', on=common_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phd_main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
